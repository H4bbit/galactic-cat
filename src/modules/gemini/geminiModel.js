const { GoogleGenerativeAI } = require("@google/generative-ai");
const fs = require("fs");
const path = require("path");
const logger = require("../../utils/logger");

require("dotenv").config();

const genAI = new GoogleGenerativeAI(process.env.GEMINI_APIKEY);

const historyFilePath = path.join(__dirname, "chatHistory.json");

/* fun√ß√£o respons√°vel por carregar o hist√≥rico de conversas do usu√°rio
a partir do arquivo chatHistory.json. Caso o arquivo n√£o exista, a fun√ß√£o
retorna um objeto vazio. */
function loadChatHistory(userId) {
  if (fs.existsSync(historyFilePath)) {
    const data = fs.readFileSync(historyFilePath, "utf8");
    const historyData = JSON.parse(data);
    logger.info("[ GEMINI MODEL ] carregando historico do usuario...");
    return historyData[userId] || { history: [], systemInstruction: null };
  }
  return { history: [], systemInstruction: null };
}

/* fun√ß√£o respons√°vel por salvar o hist√≥rico de conversas do usu√°rio no
arquivo chatHistory.json. O hist√≥rico √© salvo em um objeto com a chave
sendo o id do usu√°rio e o valor sendo um objeto com as chaves history e
systemInstruction. */
function saveChatHistory(userId, history, systemInstruction) {
  let data = {};
  if (fs.existsSync(historyFilePath)) {
    data = JSON.parse(fs.readFileSync(historyFilePath, "utf8"));
  }
  data[userId] = { history, systemInstruction };
  logger.info("[ GEMINI MODEL ] salvando historico do usuario...");
  fs.writeFileSync(historyFilePath, JSON.stringify(data, null, 2));
}

/* fun√ß√£o respons√°vel por deletar o hist√≥rico de conversas do usu√°rio
a partir do arquivo chatHistory.json. */
function deleteUserHistory(userId) {
  if (fs.existsSync(historyFilePath)) {
    const data = JSON.parse(fs.readFileSync(historyFilePath, "utf8"));
    delete data[userId];
    logger.warn("[ GEMINI MODEL ] deletando historico do usuario...");
    fs.writeFileSync(historyFilePath, JSON.stringify(data, null, 2));
  }
}

/* fun√ß√£o respons√°vel por gerar o conte√∫do de resposta do modelo de IA
a partir de um prompt fornecido pelo usu√°rio. O prompt √© enviado para o
modelo de IA, que gera uma resposta com base no hist√≥rico de conversas
do usu√°rio e na instru√ß√£o do sistema. */
async function generateAIContent(sender, prompt) {
  const helpText = `
*üëã Bem-vindo ao m√≥dulo Gemini!*

Este m√≥dulo permite que voc√™ interaja com um modelo de IA generativo de forma personalizada. Veja os comandos dispon√≠veis:  

üîπ *\`.cat <texto>\`*
Gera uma resposta de IA com base no seu hist√≥rico e na personalidade definida.  

üîπ *\`--ps <texto>\`*
Define uma instru√ß√£o de sistema personalizada para o modelo de IA. Use este comando para ajustar a personalidade ou o comportamento da IA conforme desejar.  

> *Exemplo:*
> \`.cat --ps Voc√™ √© um comediante brasileiro famoso por suas piadas r√°pidas e inteligentes.\`  

üîπ *\`--lp\`*
Limpa todo o hist√≥rico de intera√ß√µes e instru√ß√µes personalizadas do usu√°rio. ‚ö†Ô∏è *Essa a√ß√£o √© irrevers√≠vel!*  

> *Exemplo:*
> \`.cat Apague meu hist√≥rico --lp\`  

üìå *Nota:* Ap√≥s usar \`--ps\`, execute o comando novamente para aplicar as altera√ß√µes.  

üöÄ *Aproveite a experi√™ncia com o m√≥dulo Gemini!*`;

  if (typeof prompt !== "string" || prompt.trim() === "") {
    return helpText;
  }

  if (prompt.includes("--help")) {
    return helpText;
  }

  let { history, systemInstruction } = loadChatHistory(sender);
  history = history || [];
  systemInstruction = systemInstruction || "Responda sempre em portugu√™s de forma objetiva e direta, sem explica√ß√µes desnecess√°rias.";

  const psIndex = prompt.indexOf("--ps");
  if (psIndex !== -1) {
    const userInstruction = prompt.substring(psIndex + 4).trim();
    if (userInstruction) {
      systemInstruction = userInstruction;
      prompt = prompt.substring(0, psIndex).trim();
    }
    saveChatHistory(sender, history, systemInstruction);
    return "‚úÖ Instru√ß√£o do sistema atualizada com sucesso! Chame o comando novamente para aplicar as atualiza√ß√µes. üöÄ";
  }

  const lpIndex = prompt.indexOf("--lp");
  if (lpIndex !== -1) {
    deleteUserHistory(sender);
    return "‚úÖ Hist√≥rico do usu√°rio apagado com sucesso! üöÆ Chame o comando novamente para aplicar as atualiza√ß√µes. üòâ";
  }

  const model = genAI.getGenerativeModel({
    model: "gemini-1.5-flash",
    systemInstruction,
  });

  history.push({ role: "user", parts: [{ text: prompt }] });

  const chat = model.startChat({ history });
  const result = await chat.sendMessage(prompt);
  logger.info("[ GEMINI MODEL ] gerando resposta do modelo...");

  history.push({ role: "model", parts: [{ text: result.response.text() }] });
  saveChatHistory(sender, history, systemInstruction);

  return result.response.text();
}

module.exports = { generateAIContent };
